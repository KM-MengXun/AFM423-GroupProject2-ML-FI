---
title: "Untitled"
output: html_document
date: "2025-04-09"
---

Install any packages that will be used for this project
```{r, message = FALSE, warning = FALSE}
if(!require(readxl)){install.packages("readxl")}
library(readxl)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(lubridate)){install.packages("lubridate")}
library(lubridate)
if(!require(tidyverse)){install.packages("tidyverse")}
library(tidyverse)
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
if (!require(keras)) install.packages("keras")
library(keras)
```

\text{========================================================================}\
Data Wrangling\
Read the data in to the environment
```{r, message = FALSE, warning = FALSE}
# Use local drive address to load the data
load("F:/Waterloo/AFM/AFM 423/data_ml.RData")                   # Load the data
head(data_ml, 6)
```
The first column is a unique identifier for each stock and the second column is the observation date. The other columns are financial or fundamental features for example, Advt_12M_Usd would represents average trading volume over the past 12 months (in USD).\

Cleanning the data
```{r, message = FALSE, warning = FALSE}
data_ml <- data_ml %>%
  distinct() %>% #remove duplicates
  filter(date > "1999-12-31",         # Keep the date with sufficient data points
         date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```

Now we are going to create a new column "target_return" to store our predicted future stock returns
```{r, message = FALSE, warning = FALSE}
data_ml <- data_ml %>%
  mutate(target_return = R1M_Usd) %>%
  filter(!is.na(target_return))  # Remove rows without a future return
```

Then We define our train samples as the following:\
Training set: from 2000-01-01 to 2013-12-31\
Validation set: from 2014-01-01 to 2016-12-31\
Testing Set: from 2017-01-01 to 2018-12-31\
```{r, message = FALSE, warning = FALSE}
training_set <- filter(data_ml, date < as.Date("2014-01-15"))
validation_set <- filter(data_ml, (date >= as.Date("2014-01-15") 
                                   & date < as.Date("2017-01-15")))
testing_set <- filter(data_ml, date >= as.Date("2017-01-15"))
```

<!-- Scaling required? -->
Prepare data matrices
```{r, message = FALSE, warning = FALSE}
features <- training_set %>%
  select(-stock_id, -date, -R1M_Usd, -R3M_Usd, -R6M_Usd, -R12M_Usd, -target_return) %>%
  colnames()

# Step 2b: Create X and y matrices
X_train <- as.matrix(training_set[, features])
y_train <- training_set$target_return

X_val <- as.matrix(validation_set[, features])
y_val <- validation_set$target_return

X_test <- as.matrix(testing_set[, features])
y_test <- testing_set$target_return
```

Train Lasso with cross-validation
```{r, message = FALSE, warning = FALSE}
# Use 10-fold cross-validation to find optimal lambda
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)

# Plot cross-validation curve
plot(cv_lasso)

# Get best lambda
best_lambda <- cv_lasso$lambda.min
cat("Best lambda:", best_lambda, "\n")

```
The graph shows a relationship between log(lambda) and its corresponding MSE. The lowest lambda we got is 2.461288e-05\


Evaluate model on validation
```{r, message = FALSE, warning = FALSE}
# Predict on validation and test sets
pred_val <- predict(cv_lasso, s = best_lambda, newx = X_val)
pred_test <- predict(cv_lasso, s = best_lambda, newx = X_test)

# Compute RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

cat("Validation RMSE:", rmse(y_val, pred_val), "\n")
cat("Test RMSE:", rmse(y_test, pred_test), "\n")

```


Extract Lasso Features (non-zero coefficients)
```{r, message = FALSE, warning = FALSE}
# Extract coefficients at best lambda
lasso_coef <- coef(cv_lasso, s = best_lambda)
selected_features <- rownames(lasso_coef)[which(lasso_coef[, 1] != 0)]
selected_features <- setdiff(selected_features, "(Intercept)")  # remove intercept
print(selected_features)

```
These are the features that out Lasso model thinks matter for predicting the futrue return

<!-- The output can inform factor construction for building portfolios -->


Prepare the data needed for NN
```{r, message = FALSE, warning = FALSE}
# Prepare training, validation, and test sets using only selected LASSO features
X_train_nn <- as.matrix(training_set[, selected_features])
X_val_nn   <- as.matrix(validation_set[, selected_features])
X_test_nn  <- as.matrix(testing_set[, selected_features])

y_train_nn <- training_set$target_return
y_val_nn   <- validation_set$target_return
y_test_nn  <- testing_set$target_return

```

Build & Train Neural Network
```{r, message = FALSE, warning = FALSE}
# Build model
# Starts a new model using a sequential stack of layers.
# Layer 1.  32 neurons in the first hidden layer.
#     Applies ReLU activation (max(0, x)), good for non-linearity.
#     Number of input features (i.e., number of selected LASSO predictors).
# Layer 2.  16 neurons, again with ReLU.
# Layer 3.  1 output neuron: predicting a return.        
model <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu", input_shape = ncol(X_train_nn)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)  # output layer

# Compile model
# Mean Squared Error is the objective to minimize (standard for regression).
# Uses the Adam optimizer, which is adaptive and works well with most problems.
# Also tracks MSE while training, for reporting
model %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = list("mean_squared_error")
)

# Train model
# Train the model for 100 full passes through the data.
# Use mini-batches of 128 rows for updates (tradeoff between speed and stability).
# Monitors performance on validation set at the end of each epoch.
history <- model %>% fit(
  x = X_train_nn,
  y = y_train_nn,
  epochs = 100,
  batch_size = 128,
  validation_data = list(X_val_nn, y_val_nn),
  verbose = 1
)

```

Evaluate on Test Set
```{r, message = FALSE, warning = FALSE}
# Predict and calculate RMSE
pred_nn <- model %>% predict(X_test_nn)

rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

cat("Neural Network Test RMSE:", rmse(y_test_nn, pred_nn), "\n")

```

