---
title: "Untitled"
output: html_document
date: "2025-04-09"
---

Install any packages that will be used for this project
```{r, message = FALSE, warning = FALSE}
if(!require(readxl)){install.packages("readxl")}
library(readxl)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(lubridate)){install.packages("lubridate")}
library(lubridate)
if(!require(tidyverse)){install.packages("tidyverse")}
library(tidyverse)
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
if (!require(keras)) install.packages("keras")
library(keras)
if (!require(elasticnet)) install.packages("elasticnet")
library(elasticnet)

```

\text{========================================================================}\
Data Wrangling\
Read the data in to the environment
```{r, message = FALSE, warning = FALSE}
# Use local drive address to load the data
load("F:/Waterloo/AFM/AFM 423/data_ml.RData")                   # Load the data
head(data_ml, 6)
```
The first column is a unique identifier for each stock and the second column is the observation date. The other columns are financial or fundamental features for example, Advt_12M_Usd would represents average trading volume over the past 12 months (in USD).\

Cleanning the data
```{r, message = FALSE, warning = FALSE}
data_ml <- data_ml %>%
  distinct() %>% #remove duplicates
  filter(date > "1999-12-31",         # Keep the date with sufficient data points
         date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```

Now we are going to create a new column "target_return" to store our predicted future stock returns
```{r, message = FALSE, warning = FALSE}
data_ml <- data_ml %>%
  mutate(target_return = R1M_Usd) %>%
  filter(!is.na(target_return))  # Remove rows without a future return
```

Then We define our train samples as the following:\
Training set: from 2000-01-01 to 2013-12-31\
Validation set: from 2014-01-01 to 2016-12-31\
Testing Set: from 2017-01-01 to 2018-12-31\
```{r, message = FALSE, warning = FALSE}
training_set <- filter(data_ml, date < as.Date("2014-01-15"))
validation_set <- filter(data_ml, (date >= as.Date("2014-01-15") 
                                   & date < as.Date("2017-01-15")))
testing_set <- filter(data_ml, date >= as.Date("2017-01-15"))
```

<!-- Scaling required? -->
Prepare data matrices
```{r, message = FALSE, warning = FALSE}
features <- training_set %>%
  select(-stock_id, -date, -R1M_Usd, -R3M_Usd, -R6M_Usd, -R12M_Usd, -target_return) %>%
  colnames()

# Step 2b: Create X and y matrices
X_train <- as.matrix(training_set[, features])
y_train <- training_set$target_return

X_val <- as.matrix(validation_set[, features])
y_val <- validation_set$target_return

X_test <- as.matrix(testing_set[, features])
y_test <- testing_set$target_return
```

Train Lasso with cross-validation
```{r, message = FALSE, warning = FALSE}
# Use 10-fold cross-validation to find optimal lambda
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)

# Plot cross-validation curve
plot(cv_lasso)

# Get best lambda
best_lambda <- cv_lasso$lambda.min
cat("Best lambda:", best_lambda, "\n")

```
The graph shows a relationship between log(lambda) and its corresponding MSE. The lowest lambda we got is 2.461288e-05\


Evaluate model on validation
```{r, message = FALSE, warning = FALSE}
# Predict on validation and test sets
pred_val <- predict(cv_lasso, s = best_lambda, newx = X_val)
pred_test <- predict(cv_lasso, s = best_lambda, newx = X_test)

# Compute RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

cat("Validation RMSE:", rmse(y_val, pred_val), "\n")
cat("Test RMSE:", rmse(y_test, pred_test), "\n")

```


Extract Lasso Features (non-zero coefficients)
```{r, message = FALSE, warning = FALSE}
# Extract coefficients at best lambda
lasso_coef <- coef(cv_lasso, s = best_lambda)
selected_features <- rownames(lasso_coef)[which(lasso_coef[, 1] != 0)]
selected_features <- setdiff(selected_features, "(Intercept)")  # remove intercept
print(selected_features)
```
These are the features that out Lasso model thinks matter for predicting the futrue return

<!-- The output can inform factor construction for building portfolios -->


Prepare the data needed for NN
```{r, message = FALSE, warning = FALSE}
# Prepare training, validation, and test sets using only selected LASSO features
X_train_nn <- as.matrix(training_set[, selected_features])
X_val_nn   <- as.matrix(validation_set[, selected_features])
X_test_nn  <- as.matrix(testing_set[, selected_features])

y_train_nn <- training_set$target_return
y_val_nn   <- validation_set$target_return
y_test_nn  <- testing_set$target_return

```

Build & Train Neural Network
```{r, message = FALSE, warning = FALSE}
# Build model
# Starts a new model using a sequential stack of layers.
# Layer 1.  32 neurons in the first hidden layer.
#     Applies ReLU activation (max(0, x)), good for non-linearity.
#     Number of input features (i.e., number of selected LASSO predictors).
# Layer 2.  16 neurons, again with ReLU.
# Layer 3.  1 output neuron: predicting a return.        
model <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu", input_shape = ncol(X_train_nn)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)  # output layer

# Compile model
# Mean Squared Error is the objective to minimize (standard for regression).
# Uses the Adam optimizer, which is adaptive and works well with most problems.
# Also tracks MSE while training, for reporting
model %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = list("mean_squared_error")
)

# Train model
# Train the model for 100 full passes through the data.
# Use mini-batches of 128 rows for updates (tradeoff between speed and stability).
# Monitors performance on validation set at the end of each epoch.
history <- model %>% fit(
  x = X_train_nn,
  y = y_train_nn,
  epochs = 100,
  batch_size = 128,
  validation_data = list(X_val_nn, y_val_nn),
  verbose = 0
)
plot(history)

```

Evaluate on Test Set
```{r, message = FALSE, warning = FALSE}
# Predict and calculate RMSE
pred_nn <- model %>% predict(X_test_nn)

rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

cat("Neural Network Test RMSE:", rmse(y_test_nn, pred_nn), "\n")

```

```{r}
plot(y_test_nn, pred_nn,
     xlab = "Actual Return",
     ylab = "Predicted Return",
     main = "NN: Actual vs Predicted",
     pch = 19, col = rgb(0,0,1,0.4))
```

We see most returns are clustering around 0 as expected in real stock data. For some points with actual return close to 0, the model predicts much higher values from 1% to 3%, this may suggests the model might be capturing noise in some input combinations. We also see that for stock with actual return greater than 5%, the predictions stay flay between 0 and 0.5% which may indicate that our model does not extrapolate well to large moves. 




\text{========================================================================}\
```{r}
library(elasticnet)

# Step 1: Get matrix of already-scaled training features
X_spca <- as.matrix(training_set[, features])

# Step 2: Run SPCA (sparse PCA) to get 5 components
spca_result <- suppressMessages(spca(X_spca, 
                    K = 5,                      # Number of components
                    type = "predictor", 
                    sparse = "penalty", 
                    para = rep(0.4, 5),         # Sparsity level per component
                    trace = FALSE))
print(spca_result$loadings)
```
Start NN under SPCA model
```{r}
# Project components onto all sets (use same feature columns)
X_val_spca <- as.matrix(validation_set[, features])
X_test_spca <- as.matrix(testing_set[, features])

Z_train <- X_spca %*% spca_result$loadings
Z_val   <- X_val_spca %*% spca_result$loadings
Z_test  <- X_test_spca %*% spca_result$loadings

```

```{r}
# Build model using SPCA inputs (e.g., 5 components)
model_spca <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu", input_shape = ncol(Z_train)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)  # output layer for regression

# Compile model
model_spca %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = list("mean_squared_error")
)

```


```{r}
history_spca <- model_spca %>% fit(
  x = Z_train,
  y = y_train_nn,
  epochs = 100,
  batch_size = 128,
  validation_data = list(Z_val, y_val_nn),
  verbose = 0  # quiet training
)

# Optional: plot loss curve
plot(history_spca)

```

```{r}
# Predict on test set
pred_spca <- model_spca %>% predict(Z_test)

# Calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

cat("SPCA + NN Test RMSE:", rmse(y_test_nn, pred_spca), "\n")

```

```{r}
plot(y_test_nn, pred_spca,
     xlab = "Actual Return",
     ylab = "Predicted Return",
     main = "SPCA + NN: Actual vs Predicted",
     pch = 19, col = rgb(0, 0, 1, 0.4))
```

