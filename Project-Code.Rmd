---
title: "Untitled"
output: html_document
date: "2025-04-09"
---

Install any packages that will be used for this project
```{r, message = FALSE, warning = FALSE}
if(!require(readxl)){install.packages("readxl")}
library(readxl)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(lubridate)){install.packages("lubridate")}
library(lubridate)
if(!require(tidyverse)){install.packages("tidyverse")}
library(tidyverse)
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
if (!require(keras)) install.packages("keras")
library(keras)
if (!require(elasticnet)) install.packages("elasticnet")
library(elasticnet)
if (!require(tidyr)) install.packages("tidyr")
library(tidyr)
```

\text{========================================================================}\
Data Wrangling\
Read the data in to the environment
```{r, message = FALSE, warning = FALSE}
# Use local drive address to load the data
load("F:/Waterloo/AFM/AFM 423/data_ml.RData") 
# preview first few rows of the dataset
head(data_ml, 6)
```
The first column is a unique identifier for each stock and the second column is the observation date. The other columns are financial or fundamental features for example, Advt_12M_Usd would represents average trading volume over the past 12 months (in USD).\

Cleanning the data
```{r, message = FALSE, warning = FALSE}
# Clean and sort the dataset
data_ml <- data_ml %>%
  distinct() %>% #remove duplicates
  filter(date > "1999-12-31",         # Keep the date with sufficient data points
         date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```

Now we are going to create a new column "target_return" to store our predicted future stock returns
```{r, message = FALSE, warning = FALSE}
data_ml <- data_ml %>%
  mutate(target_return = R1M_Usd) %>%
  filter(!is.na(target_return))  # Remove rows without a future return
```

Then We define our train samples as the following:\
Training set: from 2000-01-01 to 2013-12-31\
Validation set: from 2014-01-01 to 2016-12-31\
Testing Set: from 2017-01-01 to 2018-12-31\
```{r, message = FALSE, warning = FALSE}
# Split into training, validation, and test sets
training_set <- filter(data_ml, date < as.Date("2014-01-15"))
validation_set <- filter(data_ml, (date >= as.Date("2014-01-15") 
                                   & date < as.Date("2017-01-15")))
testing_set <- filter(data_ml, date >= as.Date("2017-01-15"))
```


Prepare data matrices
```{r, message = FALSE, warning = FALSE}
# Define features to be used for LASSO and NN
features <- training_set %>%
  select(-stock_id, -date, -R1M_Usd, -R3M_Usd, -R6M_Usd, -R12M_Usd, -target_return) %>%
  colnames()

# Create matrix inputs for glmnet
X_train <- as.matrix(training_set[, features])
y_train <- training_set$target_return

X_val <- as.matrix(validation_set[, features])
y_val <- validation_set$target_return

X_test <- as.matrix(testing_set[, features])
y_test <- testing_set$target_return
```

Train Lasso with cross-validation
```{r, message = FALSE, warning = FALSE}
# Train LASSO model with cross-validation
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)

# plot CV error vs. lambda
plot(cv_lasso)

# Choose lambda with lowest CV error
best_lambda <- cv_lasso$lambda.min
cat("Best lambda:", best_lambda, "\n")

```
The graph shows a relationship between log(lambda) and its corresponding MSE. The lowest lambda we got is 2.461288e-05\


Evaluate model on validation
```{r, message = FALSE, warning = FALSE}
# Predict and evaluate
pred_val <- predict(cv_lasso, s = best_lambda, newx = X_val)
pred_test <- predict(cv_lasso, s = best_lambda, newx = X_test)

# Compute RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

cat("Validation RMSE:", rmse(y_val, pred_val), "\n")
cat("Test RMSE:", rmse(y_test, pred_test), "\n")

```


Extract Lasso Features (non-zero coefficients)
```{r, message = FALSE, warning = FALSE}
# Extract coefficients at best lambda
lasso_coef <- coef(cv_lasso, s = best_lambda)
selected_features <- rownames(lasso_coef)[which(lasso_coef[, 1] != 0)]
selected_features <- setdiff(selected_features, "(Intercept)")  # remove intercept
print(selected_features)
```
These are the features that out Lasso model thinks matter for predicting the future return.\


Prepare the data needed for NN
```{r, message = FALSE, warning = FALSE}
# Prepare training, validation, and test sets using only selected LASSO features
X_train_nn <- as.matrix(training_set[, selected_features])
X_val_nn   <- as.matrix(validation_set[, selected_features])
X_test_nn  <- as.matrix(testing_set[, selected_features])

y_train_nn <- training_set$target_return
y_val_nn   <- validation_set$target_return
y_test_nn  <- testing_set$target_return

```

Build & Train Neural Network
```{r, message = FALSE, warning = FALSE}
# Build model
# Starts a new model using a sequential stack of layers.
# Layer 1.  32 neurons in the first hidden layer.
#     Applies ReLU activation (max(0, x)), good for non-linearity.
#     Number of input features (i.e., number of selected LASSO predictors).
# Layer 2.  16 neurons, again with ReLU.
# Layer 3.  1 output neuron: predicting a return.        
model <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu", input_shape = ncol(X_train_nn)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)  # output layer

# Compile model
# Mean Squared Error is the objective to minimize (standard for regression).
# Uses the Adam optimizer, which is adaptive and works well with most problems.
# Also tracks MSE while training, for reporting
model %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = list("mean_squared_error")
)

# Train model
# Train the model for 100 full passes through the data.
# Use mini-batches of 128 rows for updates (tradeoff between speed and stability).
# Monitors performance on validation set at the end of each epoch.
history <- model %>% fit(
  x = X_train_nn,
  y = y_train_nn,
  epochs = 100,
  batch_size = 128,
  validation_data = list(X_val_nn, y_val_nn),
  verbose = 0
)
plot(history)

```

Evaluate on Test Set
```{r, message = FALSE, warning = FALSE}
# Predict and calculate RMSE
pred_nn <- model %>% predict(X_test_nn)

rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

cat("Neural Network Test RMSE:", rmse(y_test_nn, pred_nn), "\n")

```

```{r}
plot(y_test_nn, pred_nn,
     xlab = "Actual Return",
     ylab = "Predicted Return",
     main = "NN: Actual vs Predicted",
     pch = 19, col = rgb(0,0,1,0.4))
```

We see most returns are clustering around 0 as expected in real stock data. For some points with actual return close to 0, the model predicts much higher values from 1% to 3%, this may suggests the model might be capturing noise in some input combinations. We also see that for stock with actual return greater than 5%, the predictions stay flay between 0 and 0.5% which may indicate that our model does not extrapolate well to large moves. 



\newpage
\text{========================================================================}\
```{r}
library(elasticnet)
# Run SPCA for dimensionality reduction
X_spca <- as.matrix(training_set[, features])

spca_result <- suppressMessages(spca(X_spca, 
                    K = 5,                      # Number of components
                    type = "predictor", 
                    sparse = "penalty", 
                    para = rep(0.4, 5),         # Sparsity level per component
                    trace = FALSE))
print(spca_result$loadings)
```
Start NN under SPCA model
```{r}
# Project SPCA components
X_val_spca <- as.matrix(validation_set[, features])
X_test_spca <- as.matrix(testing_set[, features])

Z_train <- X_spca %*% spca_result$loadings
Z_val   <- X_val_spca %*% spca_result$loadings
Z_test  <- X_test_spca %*% spca_result$loadings

```

```{r}
# Train NN on SPCA components
model_spca <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu", input_shape = ncol(Z_train)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)  # output layer for regression

# Compile model
model_spca %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = list("mean_squared_error")
)

```


```{r}
history_spca <- model_spca %>% fit(
  x = Z_train,
  y = y_train_nn,
  epochs = 100,
  batch_size = 128,
  validation_data = list(Z_val, y_val_nn),
  verbose = 0  # quiet training
)

plot(history_spca)

```

```{r}
# Predict on test set
pred_spca <- model_spca %>% predict(Z_test)

# Calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

cat("SPCA + NN Test RMSE:", rmse(y_test_nn, pred_spca), "\n")

```

```{r}
plot(y_test_nn, pred_spca,
     xlab = "Actual Return",
     ylab = "Predicted Return",
     main = "SPCA + NN: Actual vs Predicted",
     pch = 19, col = rgb(0, 0, 1, 0.4))
```

\newpage
Portfolio Construction
```{r}
# Add predictions back to test set
testing_set_lasso <- testing_set %>%
  mutate(pred_return = as.numeric(pred_nn),
         model = "LASSO_NN")

testing_set_spca <- testing_set %>%
  mutate(pred_return = as.numeric(pred_spca),
         model = "SPCA_NN")

# Combine into one dataset
portfolio_data <- bind_rows(testing_set_lasso, testing_set_spca)

# Rank stocks by predicted return within each date and model
portfolio_data <- portfolio_data %>%
  group_by(model, date) %>%
  mutate(rank = percent_rank(pred_return)) %>%
  ungroup()

# Label long-short portfolio buckets
portfolio_data <- portfolio_data %>%
  mutate(port_group = case_when(
    rank >= 0.9 ~ "Top 10%",      # Long
    rank <= 0.1 ~ "Bottom 10%",   # Short
    TRUE ~ "Middle"
  ))

# Calculate average return per group
portfolio_returns <- portfolio_data %>%
  filter(port_group != "Middle") %>%
  group_by(model, date, port_group) %>%
  summarise(avg_actual_return = mean(target_return), .groups = "drop") %>%
  pivot_wider(names_from = port_group, values_from = avg_actual_return) %>%
  mutate(long_short_return = `Top 10%` - `Bottom 10%`)

```

```{r}
# Summary stats
performance_summary <- portfolio_returns %>%
  group_by(model) %>%
  summarise(
    avg_monthly_return = mean(long_short_return, na.rm = TRUE),
    sd_monthly_return = sd(long_short_return, na.rm = TRUE),
    sharpe_ratio = avg_monthly_return / sd_monthly_return,
    .groups = "drop"
  )

print(performance_summary)

```

Do a top-20% only long-only portfolio instead of long-short
```{r}
# Add predictions back to test set (already done if continuing)
testing_set_lasso <- testing_set %>%
  mutate(pred_return = as.numeric(pred_nn),
         model = "LASSO_NN")

testing_set_spca <- testing_set %>%
  mutate(pred_return = as.numeric(pred_spca),
         model = "SPCA_NN")

# Combine both models
portfolio_data <- bind_rows(testing_set_lasso, testing_set_spca)

# Rank stocks within each date and model
portfolio_data <- portfolio_data %>%
  group_by(model, date) %>%
  mutate(rank = percent_rank(pred_return)) %>%
  ungroup()

# Filter top 20% (long-only portfolio)
top_20_data <- portfolio_data %>%
  filter(rank >= 0.8)

# Calculate average monthly return
long_only_returns <- top_20_data %>%
  group_by(model, date) %>%
  summarise(top20_return = mean(target_return), .groups = "drop")

# Performance summary
long_only_summary <- long_only_returns %>%
  group_by(model) %>%
  summarise(
    avg_monthly_return = mean(top20_return, na.rm = TRUE),
    sd_monthly_return = sd(top20_return, na.rm = TRUE),
    sharpe_ratio = avg_monthly_return / sd_monthly_return,
    .groups = "drop"
  )

print(long_only_summary)
```

