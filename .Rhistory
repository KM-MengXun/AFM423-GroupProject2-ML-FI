# Train
history <- model %>% fit(
x = Z_train_spca,
y = y_train_nn_spca,
epochs = 100,
batch_size = bs,
validation_data = list(Z_val_spca, y_val_nn_spca),
verbose = 0
)
# Get final validation MSE
val_mse <- tail(history$metrics$val_mean_squared_error, 1)
# Record result
results <- rbind(results, data.frame(
units1 = units1,
units2 = units2,
learning_rate = lr,
batch_size = bs,
val_mse = val_mse
))
}
}
}
}
if(!require(readxl)){install.packages("readxl")}
library(readxl)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(lubridate)){install.packages("lubridate")}
library(lubridate)
if(!require(tidyverse)){install.packages("tidyverse")}
library(tidyverse)
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
if (!require(keras)) install.packages("keras")
library(keras)
if (!require(elasticnet)) install.packages("elasticnet")
library(elasticnet)
if (!require(tidyr)) install.packages("tidyr")
library(tidyr)
if (!require(quantmod)) install.packages("quantmod")
library(quantmod)
if (!require(tidyquant)) install.packages("tidyquant")
library(tidyquant)
set.seed(123)
tensorflow::tf$random$set_seed(123)
# Use local drive address to load the data
load("F:/Waterloo/AFM/AFM 423/data_ml.RData")
# preview first few rows of the dataset
head(data_ml, 6)
# Download SP500 index (^GSPC) from Yahoo Finance
# Get daily data first
sp500_data <- tq_get("^GSPC",
from = "2016-12-31",
to = "2018-12-31",
get = "stock.prices")
# Calculate monthly simple returns (not log returns for consistency)
sp500_monthly_returns <- sp500_data %>%
tq_transmute(
select = adjusted,
mutate_fun = periodReturn,
period = "monthly",
type = "arithmetic"  # simple returns
) %>%
rename(sp500_return = monthly.returns)
# Clean and sort the dataset
target_leakage <- c("R3M_Usd", "R6M_Usd", "R12M_Usd")
data_ml <- data_ml %>%
distinct() %>% #remove duplicates
filter(date > "1999-12-31",         # Keep the date with sufficient data points
date < "2019-01-01") %>%
select(-all_of(target_leakage)) %>% # remove predictors that may cause target leakage
arrange(stock_id, date)             # Order the data
data_ml <- data_ml %>%
mutate(target_return = R1M_Usd) %>%
filter(!is.na(target_return))  # Remove rows without a future return
# verify predictors are all scaled (standardized)
col_mean <- apply(data_ml[, 3:ncol(data_ml)], 2, mean)
col_sd <- apply(data_ml[, 3:ncol(data_ml)], 2, sd)
standardization_check <- data.frame(Column = colnames(data_ml[, 3:ncol(data_ml)]),
mean = col_mean,
sd = col_sd)
mean_range <- c(min(standardization_check$mean), max(standardization_check$mean))
sd_range <- c(min(standardization_check$sd), max(standardization_check$sd))
cat('Mean range:', mean_range[1], '-', mean_range[2], '\n')
cat('Standard Deviation range:', sd_range[1], '-', sd_range[2], '\n')
# standardize the predictors
data_ml <- data_ml %>% mutate(across(
.cols = -all_of(c('stock_id', 'date', 'R1M_Usd', 'target_return')),
.fns = ~scale(.)[,1]
))
# Split into training, validation, and test sets
training_set <- filter(data_ml, date < as.Date("2014-01-15"))
validation_set <- filter(data_ml, (date >= as.Date("2014-01-15")
& date < as.Date("2017-01-15")))
testing_set <- filter(data_ml, date >= as.Date("2017-01-15"))
# Define features to be used for LASSO and NN
features <- training_set %>%
select(-stock_id, -date, -R1M_Usd, -target_return) %>%
colnames()
# Create matrix inputs for glmnet
X_train <- as.matrix(training_set[, features])
y_train <- training_set$target_return
X_val <- as.matrix(validation_set[, features])
y_val <- validation_set$target_return
X_test <- as.matrix(testing_set[, features])
y_test <- testing_set$target_return
# Train LASSO model with cross-validation
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)
# plot CV error vs. lambda
plot(cv_lasso)
plot(cv_lasso, ylim=c(0.0285, 0.0295))
# Choose lambda with lowest CV error
best_lambda <- cv_lasso$lambda.min
cat("Best lambda:", best_lambda, "\n")
# Extract coefficients at best lambda
lasso_coef <- coef(cv_lasso, s = best_lambda)
selected_features <- rownames(lasso_coef)[which(lasso_coef[, 1] != 0)]
selected_features <- setdiff(selected_features, "(Intercept)")  # remove intercept
lasso_coef <- as.matrix(lasso_coef) %>%
as.data.frame() %>%
slice(-1) %>%
filter(s1 !=0) %>%
arrange(desc(abs(s1)))
lasso_coef
# Prepare training, validation, and test sets using only selected LASSO features
X_train_nn_lasso <- as.matrix(training_set[, selected_features])
X_val_nn_lasso   <- as.matrix(validation_set[, selected_features])
X_test_nn_lasso  <- as.matrix(testing_set[, selected_features])
y_train_nn_lasso <- training_set$target_return
y_val_nn_lasso   <- validation_set$target_return
y_test_nn_lasso  <- testing_set$target_return
# Prepare training, validation, and test sets using only selected LASSO features
X_train_nn_lasso <- as.matrix(training_set[, selected_features])
X_val_nn_lasso   <- as.matrix(validation_set[, selected_features])
X_test_nn_lasso  <- as.matrix(testing_set[, selected_features])
y_train_nn_lasso <- training_set$target_return
y_val_nn_lasso   <- validation_set$target_return
y_test_nn_lasso  <- testing_set$target_return
# Define a grid of hyperparameters
units1_list <- c(16, 32, 64)
units2_list <- c(8, 16, 32)
learning_rates <- c(0.001, 0.0005)
batch_sizes <- c(64, 128)
results <- data.frame()
for (units1 in units1_list) {
for (units2 in units2_list) {
for (lr in learning_rates) {
for (bs in batch_sizes) {
# Build model
model <- keras_model_sequential() %>%
layer_dense(units = units1, activation = "relu", input_shape = ncol(X_train_nn_lasso)) %>%
layer_dense(units = units2, activation = "relu") %>%
layer_dense(units = 1)
# Compile
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = lr),
metrics = list("mean_squared_error")
)
# Train
history <- model %>% fit(
x = X_train_nn_lasso,
y = y_train_nn_lasso,
epochs = 100,
batch_size = bs,
validation_data = list(X_val_nn_lasso, y_val_nn_lasso),
verbose = 0
)
# Get final validation MSE
val_mse <- tail(history$metrics$val_mean_squared_error, 1)
# Record result
results <- rbind(results, data.frame(
units1 = units1,
units2 = units2,
learning_rate = lr,
batch_size = bs,
val_mse = val_mse
))
}
}
}
}
# Find the best hyperparameters
best_result <- results[which.min(results$val_mse), ]
print(best_result)
best_result$units1
best_result$units2
best_result$learning_rate
best_result$batch_size
View(best_result)
# Build model
# Starts a new model using a sequential stack of layers.
# Layer 1.  x neurons in the first hidden layer.
#     Applies ReLU activation (max(0, x)), good for non-linearity.
#     Number of input features (i.e., number of selected LASSO predictors).
# Layer 2.  x neurons, again with ReLU.
# Layer 3.  1 output neuron: predicting a return.
model <- keras_model_sequential() %>%
layer_dense(units = best_result$units1, activation = "relu",
input_shape = ncol(X_train_nn_lasso)) %>%
layer_dense(units = best_result$units2, activation = "relu") %>%
layer_dense(units = 1)  # output layer
# Compile model
# Mean Squared Error is the objective to minimize (standard for regression).
# Uses the Adam optimizer, which is adaptive and works well with most problems.
# Also tracks MSE while training, for reporting
# Learning Rate x
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = best_result$learning_rate),
metrics = list("mean_squared_error")
)
# Train model
# Train the model for 100 full passes through the data.
# Use mini-batches of x rows for updates (tradeoff between speed and stability).
# Monitors performance on validation set at the end of each epoch.
history <- model %>% fit(
x = X_train_nn_lasso,
y = y_train_nn_lasso,
epochs = 100,
batch_size = best_result$batch_size,
validation_data = list(X_val_nn_lasso, y_val_nn_lasso),
verbose = 0
)
plot(history)
#title(main = "LASSO-Selected Features Neural Network")
# Predict and calculate RMSE
pred_nn_lasso_train <- model %>% predict(X_train_nn_lasso)
pred_nn_lasso_test <- model %>% predict(X_test_nn_lasso)
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("LASSO + NN Train RMSE:", rmse(y_train_nn_lasso, pred_nn_lasso_train), "\n")
cat("LASSO + NN Test RMSE:", rmse(y_test_nn_lasso, pred_nn_lasso_test), "\n")
plot(y_test_nn_lasso, pred_nn_lasso_test,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted",
pch = 19, col = rgb(0,0,1,0.4))
# SPCA transformed matrices for training, validation and test
X_train_spca <- as.matrix(training_set[, features])
X_val_spca <- as.matrix(validation_set[, features])
X_test_spca <- as.matrix(testing_set[, features])
y_train_nn_spca <- training_set$target_return
y_val_nn_spca  <- validation_set$target_return
y_test_nn_spca <- testing_set$target_return
# Run SPCA for dimensionality reduction
spca_result <- suppressMessages(spca(X_train_spca,
K = 5,                      # Number of components
type = "predictor",
sparse = "penalty",
para = rep(0.4, 5),         # Sparsity level per component
trace = FALSE))
print(spca_result$loadings)
# loadings < 0.05 are considered close to zero and insignficant
percentage <- mean(spca_result$loadings <0.05)*100
cat('Percentage of close to zero loadings (< 0.05):', percentage, "\n")
# SPCA transformed matrices for training, validation and test
Z_train_spca <- X_train_spca %*% spca_result$loadings
Z_val_spca   <- X_val_spca %*% spca_result$loadings
Z_test_spca  <- X_test_spca %*% spca_result$loadings
# Define a grid of hyperparameters
units1_list <- c(16, 32, 64)
units2_list <- c(8, 16, 32)
learning_rates <- c(0.001, 0.0005)
batch_sizes <- c(64, 128)
results <- data.frame()
for (units1 in units1_list) {
for (units2 in units2_list) {
for (lr in learning_rates) {
for (bs in batch_sizes) {
# Build model
model <- keras_model_sequential() %>%
layer_dense(units = units1, activation = "relu", input_shape = ncol(Z_train_spca)) %>%
layer_dense(units = units2, activation = "relu") %>%
layer_dense(units = 1)
# Compile
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = lr),
metrics = list("mean_squared_error")
)
# Train
history <- model %>% fit(
x = Z_train_spca,
y = y_train_nn_spca,
epochs = 100,
batch_size = bs,
validation_data = list(Z_val_spca, y_val_nn_spca),
verbose = 0
)
# Get final validation MSE
val_mse <- tail(history$metrics$val_mean_squared_error, 1)
# Record result
results <- rbind(results, data.frame(
units1 = units1,
units2 = units2,
learning_rate = lr,
batch_size = bs,
val_mse = val_mse
))
}
}
}
}
# Find the best hyperparameters
best_result <- results[which.min(results$val_mse), ]
print(best_result)
# Train NN on SPCA components
model_spca <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(Z_train_spca)) %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 1)  # output layer for regression
# Compile model
model_spca %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = list("mean_squared_error")
)
history_spca <- model_spca %>% fit(
x = Z_train_spca,
y = y_train_nn_spca,
epochs = 100,
batch_size = 64,
validation_data = list(Z_val_spca, y_val_nn_spca),
verbose = 0  # quiet training
)
plot(history_spca)
best_result$units1
# Train NN on SPCA components
model_spca <- keras_model_sequential() %>%
layer_dense(units = best_result$units1,
activation = "relu", input_shape = ncol(Z_train_spca)) %>%
layer_dense(units = best_result$units2, activation = "relu") %>%
layer_dense(units = 1)  # output layer for regression
# Compile model
model_spca %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = best_result$learning_rate),
metrics = list("mean_squared_error")
)
history_spca <- model_spca %>% fit(
x = Z_train_spca,
y = y_train_nn_spca,
epochs = 100,
batch_size = best_result$batch_size,
validation_data = list(Z_val_spca, y_val_nn_spca),
verbose = 0  # quiet training
)
plot(history_spca)
# Predict on test set
pred_spca_train <- model_spca %>% predict(Z_train)
# Predict on test set
pred_spca_train <- model_spca %>% predict(Z_train)
# Predict on test set
pred_spca_train <- model_spca %>% predict(Z_train)
# Predict on test set
pred_spca_train <- model_spca %>% predict(Z_train_spca)
pred_spca_test <- model_spca %>% predict(Z_test_spca)
# Calculate RMSE
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("SPCA + NN Train RMSE:", rmse(y_train_nn, pred_spca_train), "\n")
# Predict on test set
pred_spca_train <- model_spca %>% predict(Z_train_spca)
pred_spca_test <- model_spca %>% predict(Z_test_spca)
# Calculate RMSE
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("SPCA + NN Train RMSE:", rmse(y_train_spca, pred_spca_train), "\n")
# Predict on test set
pred_spca_train <- model_spca %>% predict(Z_train_spca)
pred_spca_test <- model_spca %>% predict(Z_test_spca)
# Calculate RMSE
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("SPCA + NN Train RMSE:", rmse(y_train_nn_spca, pred_spca_train), "\n")
cat("SPCA + NN Test RMSE:", rmse(y_test_nn_spca, pred_spca_test), "\n")
plot(y_test_nn_spca, pred_spca_test,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "SPCA + NN: Actual vs Predicted",
#xlim = c(-0.05, 0.05),   # Zoom on X-axis: Actual returns between -10% and +10%
# ylim = c(-0.1, 0.1),
pch = 19, col = rgb(0, 0, 1, 0.4))
plot(y_test_nn_spca, pred_spca_test,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "SPCA + NN: Actual vs Predicted",
#xlim = c(-0.05, 0.05),   # Zoom on X-axis: Actual returns between -10% and +10%
# ylim = c(-0.1, 0.1),
pch = 19, col = rgb(0, 0, 1, 0.4))
# Add predictions back to test set (already done if continuing)
testing_set_lasso <- testing_set %>%
mutate(pred_return = as.numeric(pred_nn),
model = "LASSO_NN")
# Add predictions back to test set (already done if continuing)
testing_set_lasso <- testing_set %>%
mutate(pred_return = as.numeric(pred_nn_lasso_test),
model = "LASSO_NN")
testing_set_spca <- testing_set %>%
mutate(pred_return = as.numeric(pred_spca_test),
model = "SPCA_NN")
# Combine both models
portfolio_data <- bind_rows(testing_set_lasso, testing_set_spca)
# Rank stocks within each date and model
portfolio_data <- portfolio_data %>%
group_by(model, date) %>%
mutate(rank = percent_rank(pred_return)) %>%
ungroup()
# Filter top 20% (long-only portfolio)
top_20_data <- portfolio_data %>%
filter(rank >= 0.8)
# Calculate average monthly return
long_only_returns <- top_20_data %>%
group_by(model, date) %>%
summarise(top20_return = mean(target_return), .groups = "drop")
# Performance summary
long_only_summary <- long_only_returns %>%
group_by(model) %>%
summarise(
avg_monthly_return = mean(top20_return, na.rm = TRUE),
holding_period_return = prod(1+top20_return)-1,
sd_monthly_return = sd(top20_return, na.rm = TRUE),
sharpe_ratio = avg_monthly_return / sd_monthly_return,
.groups = "drop"
)
print(long_only_summary)
# Add predictions back to test set (already done if continuing)
testing_set_lasso <- testing_set %>%
mutate(pred_return = as.numeric(pred_nn_lasso_test),
model = "LASSO_NN")
testing_set_spca <- testing_set %>%
mutate(pred_return = as.numeric(pred_spca_test),
model = "SPCA_NN")
# Combine both models
portfolio_data <- bind_rows(testing_set_lasso, testing_set_spca)
# Rank stocks within each date and model
portfolio_data <- portfolio_data %>%
group_by(model, date) %>%
mutate(rank = percent_rank(pred_return)) %>%
ungroup()
# Filter top 20% (long-only portfolio)
top_20_data <- portfolio_data %>%
filter(rank >= 0.8)
# Calculate average monthly return
long_only_returns <- top_20_data %>%
group_by(model, date) %>%
summarise(top20_return = mean(target_return), .groups = "drop")
# Performance summary
long_only_summary <- long_only_returns %>%
group_by(model) %>%
summarise(
avg_monthly_return = mean(top20_return, na.rm = TRUE),
holding_period_return = prod(1+top20_return)-1,
sd_monthly_return = sd(top20_return, na.rm = TRUE),
sharpe_ratio = avg_monthly_return / sd_monthly_return,
.groups = "drop"
)
print(long_only_summary)
# Calculate
sp500_avg_return <- mean(sp500_monthly_returns$sp500_return, na.rm = TRUE)
sp500_sd_return <- sd(sp500_monthly_returns$sp500_return, na.rm = TRUE)
sp500_holding_period_return <- prod(1+sp500_monthly_returns$sp500_return) -1
sp500_sharpe <- sp500_avg_return / sp500_sd_return
# Show result
tibble(
model = "SP500",
avg_monthly_return = sp500_avg_return,
holding_period_return = sp500_holding_period_return,
sd_monthly_return = sp500_sd_return,
sharpe_ratio = sp500_sharpe
)
# cumulative returns for three strategies
long_only_returns_lasso <- long_only_returns[long_only_returns[,'model'] == 'LASSO_NN',]
long_only_returns_spca <- long_only_returns[long_only_returns[,'model'] == 'SPCA_NN',]
# Add cumulative returns
monthly_portfolio_return_lasso <- long_only_returns_lasso %>%
mutate(cumulative_return_lasso = cumprod(1 + top20_return))
monthly_portfolio_return_spca <- long_only_returns_spca %>%
mutate(cumulative_return_spca = cumprod(1 + top20_return))
sp500_monthly_returns <- sp500_monthly_returns %>%
mutate(cumulative_return_sp500 = cumprod(1 + sp500_return))
# mutate sp500 date column to make sure it aligns with the date from lasso and spca
# Replace sp500 date with spca portfolio date
sp500_monthly_returns <- sp500_monthly_returns %>%
mutate(date = long_only_returns_lasso$date)
compare_cumulative_returns <- monthly_portfolio_return_lasso %>%
select(date, cumulative_return_lasso) %>%
left_join(monthly_portfolio_return_spca %>% select(date, cumulative_return_spca), by = "date") %>%
left_join(sp500_monthly_returns %>% select(date, cumulative_return_sp500), by = "date")
compare_cumulative_returns_long <- compare_cumulative_returns %>%
pivot_longer(
cols = starts_with("cumulative_return"),
names_to = "model",
values_to = "cumulative_return"
)
ggplot(compare_cumulative_returns_long, aes(x = date, y = cumulative_return, color = model)) +
geom_line(size = 1.2) +
labs(title = "Cumulative Returns: LASSO_NN vs SPCA_NN vs SP500",
x = "Date",
y = "Cumulative Return (Index Level)",
color = "Strategy") +
theme_minimal() +
scale_color_manual(values = c("cumulative_return_lasso" = "red",
"cumulative_return_spca" = "blue",
"cumulative_return_sp500" = "black")) +
theme(plot.title = element_text(hjust = 0.5))
View(data_ml)
