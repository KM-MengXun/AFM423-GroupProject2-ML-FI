# Use local drive address to load the data
load("data_ml.RData")                   # Load the data
# Use local drive address to load the data
load(""F:/Waterloo/AFM/AFM 423/data_ml.RData"")                   # Load the data
if(!require(readxl)){install.packages("readxl")}
library(readxl)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(lubridate)){install.packages("lubridate")}
library(lubridate)
if(!require(tidyverse)){install.packages("tidyverse")}
library(tidyverse)
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
if (!require(keras)) install.packages("keras")
library(keras)
# Use local drive address to load the data
load(F:/Waterloo/AFM/AFM 423/data_ml.RData"")                   # Load the data
# Use local drive address to load the data
load("F:/Waterloo/AFM/AFM 423/data_ml.RData"")                   # Load the data
head(data_ml, 6)
# Use local drive address to load the data
load("F:/Waterloo/AFM/AFM 423/data_ml.RData")                   # Load the data
head(data_ml, 6)
if(!require(readxl)){install.packages("readxl")}
library(readxl)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(lubridate)){install.packages("lubridate")}
library(lubridate)
if(!require(tidyverse)){install.packages("tidyverse")}
library(tidyverse)
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
if (!require(keras)) install.packages("keras")
library(keras)
# Use local drive address to load the data
load("F:/Waterloo/AFM/AFM 423/data_ml.RData")                   # Load the data
head(data_ml, 6)
data_ml <- data_ml %>%
distinct() %>% #remove duplicates
filter(date > "1999-12-31",         # Keep the date with sufficient data points
date < "2019-01-01") %>%
arrange(stock_id, date)             # Order the data
data_ml <- data_ml %>%
mutate(target_return = R1M_Usd) %>%
filter(!is.na(target_return))  # Remove rows without a future return
training_set <- filter(data_ml, date < as.Date("2014-01-15"))
validation_set <- filter(data_ml, (date >= as.Date("2014-01-15")
& date < as.Date("2017-01-15")))
testing_set <- filter(data_ml, date >= as.Date("2017-01-15"))
features <- training_set %>%
select(-stock_id, -date, -R1M_Usd, -R3M_Usd, -R6M_Usd, -R12M_Usd, -target_return) %>%
colnames()
# Step 2b: Create X and y matrices
X_train <- as.matrix(training_set[, features])
y_train <- training_set$target_return
X_val <- as.matrix(validation_set[, features])
y_val <- validation_set$target_return
X_test <- as.matrix(testing_set[, features])
y_test <- testing_set$target_return
# Use 10-fold cross-validation to find optimal lambda
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)
# Plot cross-validation curve
plot(cv_lasso)
# Get best lambda
best_lambda <- cv_lasso$lambda.min
cat("Best lambda:", best_lambda, "\n")
# Predict on validation and test sets
pred_val <- predict(cv_lasso, s = best_lambda, newx = X_val)
pred_test <- predict(cv_lasso, s = best_lambda, newx = X_test)
# Compute RMSE
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("Validation RMSE:", rmse(y_val, pred_val), "\n")
cat("Test RMSE:", rmse(y_test, pred_test), "\n")
# Extract coefficients at best lambda
lasso_coef <- coef(cv_lasso, s = best_lambda)
selected_features <- rownames(lasso_coef)[which(lasso_coef[, 1] != 0)]
selected_features <- setdiff(selected_features, "(Intercept)")  # remove intercept
print(selected_features)
# Prepare training, validation, and test sets using only selected LASSO features
X_train_nn <- as.matrix(training_set[, selected_features])
X_val_nn   <- as.matrix(validation_set[, selected_features])
X_test_nn  <- as.matrix(testing_set[, selected_features])
y_train_nn <- training_set$target_return
y_val_nn   <- validation_set$target_return
y_test_nn  <- testing_set$target_return
# Build model
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = ncol(X_train_nn)) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1)  # output layer
# Compile model
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = list("mean_squared_error")
)
# Train model
history <- model %>% fit(
x = X_train_nn,
y = y_train_nn,
epochs = 100,
batch_size = 128,
validation_data = list(X_val_nn, y_val_nn),
verbose = 1
)
# Predict and calculate RMSE
pred_nn <- model %>% predict(X_test_nn)
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("Neural Network Test RMSE:", rmse(y_test_nn, pred_nn), "\n")
# Build model
# Starts a new model using a sequential stack of layers.
# Layer 1.  32 neurons in the first hidden layer.
#     Applies ReLU activation (max(0, x)), good for non-linearity.
#     Number of input features (i.e., number of selected LASSO predictors).
# Layer 2.  16 neurons, again with ReLU.
# Layer 3.  1 output neuron: predicting a return.
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = ncol(X_train_nn)) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1)  # output layer
# Compile model
# Mean Squared Error is the objective to minimize (standard for regression).
# Uses the Adam optimizer, which is adaptive and works well with most problems.
# Also tracks MSE while training, for reporting
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = list("mean_squared_error")
)
# Train model
# Train the model for 100 full passes through the data.
# Use mini-batches of 128 rows for updates (tradeoff between speed and stability).
# Monitors performance on validation set at the end of each epoch.
history <- model %>% fit(
x = X_train_nn,
y = y_train_nn,
epochs = 100,
batch_size = 128,
validation_data = list(X_val_nn, y_val_nn),
)
# Build model
# Starts a new model using a sequential stack of layers.
# Layer 1.  32 neurons in the first hidden layer.
#     Applies ReLU activation (max(0, x)), good for non-linearity.
#     Number of input features (i.e., number of selected LASSO predictors).
# Layer 2.  16 neurons, again with ReLU.
# Layer 3.  1 output neuron: predicting a return.
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = ncol(X_train_nn)) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1)  # output layer
# Compile model
# Mean Squared Error is the objective to minimize (standard for regression).
# Uses the Adam optimizer, which is adaptive and works well with most problems.
# Also tracks MSE while training, for reporting
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = list("mean_squared_error")
)
# Train model
# Train the model for 100 full passes through the data.
# Use mini-batches of 128 rows for updates (tradeoff between speed and stability).
# Monitors performance on validation set at the end of each epoch.
history <- model %>% fit(
x = X_train_nn,
y = y_train_nn,
epochs = 100,
batch_size = 128,
validation_data = list(X_val_nn, y_val_nn),
verbose = 1
)
# Predict and calculate RMSE
pred_nn <- model %>% predict(X_test_nn)
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("Neural Network Test RMSE:", rmse(y_test_nn, pred_nn), "\n")
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted",
pch = 19, col = rgb(0,0,1,0.4))
if(!require(readxl)){install.packages("readxl")}
library(readxl)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(lubridate)){install.packages("lubridate")}
library(lubridate)
if(!require(tidyverse)){install.packages("tidyverse")}
library(tidyverse)
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
if (!require(keras)) install.packages("keras")
library(keras)
# Use local drive address to load the data
load("F:/Waterloo/AFM/AFM 423/data_ml.RData")                   # Load the data
head(data_ml, 6)
data_ml <- data_ml %>%
distinct() %>% #remove duplicates
filter(date > "1999-12-31",         # Keep the date with sufficient data points
date < "2019-01-01") %>%
arrange(stock_id, date)             # Order the data
data_ml <- data_ml %>%
mutate(target_return = R1M_Usd) %>%
filter(!is.na(target_return))  # Remove rows without a future return
training_set <- filter(data_ml, date < as.Date("2014-01-15"))
validation_set <- filter(data_ml, (date >= as.Date("2014-01-15")
& date < as.Date("2017-01-15")))
testing_set <- filter(data_ml, date >= as.Date("2017-01-15"))
features <- training_set %>%
select(-stock_id, -date, -R1M_Usd, -R3M_Usd, -R6M_Usd, -R12M_Usd, -target_return) %>%
colnames()
# Step 2b: Create X and y matrices
X_train <- as.matrix(training_set[, features])
y_train <- training_set$target_return
X_val <- as.matrix(validation_set[, features])
y_val <- validation_set$target_return
X_test <- as.matrix(testing_set[, features])
y_test <- testing_set$target_return
# Use 10-fold cross-validation to find optimal lambda
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)
# Plot cross-validation curve
plot(cv_lasso)
# Get best lambda
best_lambda <- cv_lasso$lambda.min
cat("Best lambda:", best_lambda, "\n")
# Predict on validation and test sets
pred_val <- predict(cv_lasso, s = best_lambda, newx = X_val)
pred_test <- predict(cv_lasso, s = best_lambda, newx = X_test)
# Compute RMSE
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("Validation RMSE:", rmse(y_val, pred_val), "\n")
cat("Test RMSE:", rmse(y_test, pred_test), "\n")
# Extract coefficients at best lambda
lasso_coef <- coef(cv_lasso, s = best_lambda)
selected_features <- rownames(lasso_coef)[which(lasso_coef[, 1] != 0)]
selected_features <- setdiff(selected_features, "(Intercept)")  # remove intercept
print(selected_features)
# Prepare training, validation, and test sets using only selected LASSO features
X_train_nn <- as.matrix(training_set[, selected_features])
X_val_nn   <- as.matrix(validation_set[, selected_features])
X_test_nn  <- as.matrix(testing_set[, selected_features])
y_train_nn <- training_set$target_return
y_val_nn   <- validation_set$target_return
y_test_nn  <- testing_set$target_return
# Build model
# Starts a new model using a sequential stack of layers.
# Layer 1.  32 neurons in the first hidden layer.
#     Applies ReLU activation (max(0, x)), good for non-linearity.
#     Number of input features (i.e., number of selected LASSO predictors).
# Layer 2.  16 neurons, again with ReLU.
# Layer 3.  1 output neuron: predicting a return.
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = ncol(X_train_nn)) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1)  # output layer
# Compile model
# Mean Squared Error is the objective to minimize (standard for regression).
# Uses the Adam optimizer, which is adaptive and works well with most problems.
# Also tracks MSE while training, for reporting
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = list("mean_squared_error")
)
# Train model
# Train the model for 100 full passes through the data.
# Use mini-batches of 128 rows for updates (tradeoff between speed and stability).
# Monitors performance on validation set at the end of each epoch.
history <- model %>% fit(
x = X_train_nn,
y = y_train_nn,
epochs = 100,
batch_size = 128,
validation_data = list(X_val_nn, y_val_nn),
verbose = 1
)
# Predict and calculate RMSE
pred_nn <- model %>% predict(X_test_nn)
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("Neural Network Test RMSE:", rmse(y_test_nn, pred_nn), "\n")
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted",
pch = 19, col = rgb(0,0,1,0.4))
abline(0, 1, col = "red", lwd = 2)
pred_nn
y_test_nn
View(testing_set)
testing_set$target_return
?plot
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted",
pch = 19, col = rgb(0,0,1,0.4))
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted (Zoomed -1 to 1)",
pch = 19,
col = rgb(0, 0, 1, 0.4),
xlim = c(-1, 1),
ylim = c(-1, 1))  # Set x and y axis limits
abline(0, 1, col = "red", lwd = 2)  # Reference line for perfect prediction
pred_nn
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted (Zoomed -1 to 1)",
pch = 19,
col = rgb(0, 0, 1, 0.4),
xlim = c(-1, 1),
ylim = c(-0.2, 1))  # Set x and y axis limits
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted (Zoomed -1 to 1)",
pch = 19,
col = rgb(0, 0, 1, 0.4),
xlim = c(-1, 1),
ylim = c(-1, 1))  # Set x and y axis limits
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted (Zoomed -1 to 1)",
pch = 19,
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted (Zoomed -1 to 1)"
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted ")
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted",
pch = 19, col = rgb(0,0,1,0.4))
if(!require(readxl)){install.packages("readxl")}
library(readxl)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(lubridate)){install.packages("lubridate")}
library(lubridate)
if(!require(tidyverse)){install.packages("tidyverse")}
library(tidyverse)
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
if (!require(keras)) install.packages("keras")
library(keras)
# Use local drive address to load the data
load("F:/Waterloo/AFM/AFM 423/data_ml.RData")                   # Load the data
head(data_ml, 6)
data_ml <- data_ml %>%
distinct() %>% #remove duplicates
filter(date > "1999-12-31",         # Keep the date with sufficient data points
date < "2019-01-01") %>%
arrange(stock_id, date)             # Order the data
data_ml <- data_ml %>%
mutate(target_return = R1M_Usd) %>%
filter(!is.na(target_return))  # Remove rows without a future return
training_set <- filter(data_ml, date < as.Date("2014-01-15"))
validation_set <- filter(data_ml, (date >= as.Date("2014-01-15")
& date < as.Date("2017-01-15")))
testing_set <- filter(data_ml, date >= as.Date("2017-01-15"))
features <- training_set %>%
select(-stock_id, -date, -R1M_Usd, -R3M_Usd, -R6M_Usd, -R12M_Usd, -target_return) %>%
colnames()
# Step 2b: Create X and y matrices
X_train <- as.matrix(training_set[, features])
y_train <- training_set$target_return
X_val <- as.matrix(validation_set[, features])
y_val <- validation_set$target_return
X_test <- as.matrix(testing_set[, features])
y_test <- testing_set$target_return
# Use 10-fold cross-validation to find optimal lambda
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)
# Plot cross-validation curve
plot(cv_lasso)
# Get best lambda
best_lambda <- cv_lasso$lambda.min
cat("Best lambda:", best_lambda, "\n")
# Predict on validation and test sets
pred_val <- predict(cv_lasso, s = best_lambda, newx = X_val)
pred_test <- predict(cv_lasso, s = best_lambda, newx = X_test)
# Compute RMSE
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("Validation RMSE:", rmse(y_val, pred_val), "\n")
cat("Test RMSE:", rmse(y_test, pred_test), "\n")
# Extract coefficients at best lambda
lasso_coef <- coef(cv_lasso, s = best_lambda)
selected_features <- rownames(lasso_coef)[which(lasso_coef[, 1] != 0)]
selected_features <- setdiff(selected_features, "(Intercept)")  # remove intercept
print(selected_features)
# Prepare training, validation, and test sets using only selected LASSO features
X_train_nn <- as.matrix(training_set[, selected_features])
X_val_nn   <- as.matrix(validation_set[, selected_features])
X_test_nn  <- as.matrix(testing_set[, selected_features])
y_train_nn <- training_set$target_return
y_val_nn   <- validation_set$target_return
y_test_nn  <- testing_set$target_return
# Build model
# Starts a new model using a sequential stack of layers.
# Layer 1.  32 neurons in the first hidden layer.
#     Applies ReLU activation (max(0, x)), good for non-linearity.
#     Number of input features (i.e., number of selected LASSO predictors).
# Layer 2.  16 neurons, again with ReLU.
# Layer 3.  1 output neuron: predicting a return.
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = ncol(X_train_nn)) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1)  # output layer
# Compile model
# Mean Squared Error is the objective to minimize (standard for regression).
# Uses the Adam optimizer, which is adaptive and works well with most problems.
# Also tracks MSE while training, for reporting
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = list("mean_squared_error")
)
# Train model
# Train the model for 100 full passes through the data.
# Use mini-batches of 128 rows for updates (tradeoff between speed and stability).
# Monitors performance on validation set at the end of each epoch.
history <- model %>% fit(
x = X_train_nn,
y = y_train_nn,
epochs = 100,
batch_size = 128,
validation_data = list(X_val_nn, y_val_nn),
verbose = 0
)
plot(history)
# Predict and calculate RMSE
pred_nn <- model %>% predict(X_test_nn)
rmse <- function(actual, predicted) {
sqrt(mean((actual - predicted)^2))
}
cat("Neural Network Test RMSE:", rmse(y_test_nn, pred_nn), "\n")
plot(y_test_nn, pred_nn,
xlab = "Actual Return",
ylab = "Predicted Return",
main = "NN: Actual vs Predicted",
pch = 19, col = rgb(0,0,1,0.4))
