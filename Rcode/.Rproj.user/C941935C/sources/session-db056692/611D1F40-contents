---
title: "Untitled"
output: pdf_document
date: "2025-04-03"
---
\newpage
#Q1\
The paper is mainly about an approach to latent factor modeling in asset pricing using autoencoder neural network. By implementing this method, the model will be able to capture nonlineariteis in asset returns.In their model. They started with the model proposed by KPS replace the conventional linear mapping from characteristics to factor loadings with a flexible, nonlinear specification. In their model, the traditional factor model $r_{i,t} = \beta(z_{i,t-1})'=f_t+u_{i, t}$is reinterpreted using an autoencoder architecture. The beta network—responsible for translating high-dimensional firm characteristics into asset-specific loadings—operates through multiple layers of nonlinear transformations (using activation functions such as ReLU or tanh), while the factor network compresses the cross-section of returns into a lower-dimensional representation. A dot product then connect the two streams to yield the expected asset returns. Importantly, the authors enforce the economic no-arbitrage condition by omitting an intercept in the conditional beta formulation, ensuring that all return predictability arises from risk exposures rather than mispricing.\
In order to regularized the autoencoder learning, the authors add a L1 penalth (Lasso), to the objective fucntion to encourage sparsity in the network weights which effectively makes the model simpler by driving significant weights to zero.\
The authors also consducts an empirical studys regarding the monthly individual stock returns from Center for Research in Securities Prices (CRSP) for all firms listed in the three largest stock: NYSE, AMEX, and NASDAQ. The sample has a duration of 60 years and includes 94 charateristics. Also to avoid forward-looking bias, they introduce a maximum lag differs between each periods and replace any missing characteristics with cross-sectrional median.\
By integrating these rich datasets into their conditional autoencoder framework, the authors are able to capture both the common risk factors and the nonlinear, time-varying exposures driven by firm characteristics. The model is shown to outperform traditional observable factor models in both statistical measures (higher out-of-sample total and predictive R²) and economic performance (notably higher Sharpe ratios for long–short decile spread portfolios). This suggests that incorporating dynamic, nonlinear information from firm characteristics can significantly enhance our understanding and prediction of U.S. equity returns.\
In their conclusion, they emphasize that the conditional autoencoder model outperforms both traditional static factor models and linear conditional models in explaining and predicting U.S. equity returns. \

In general, they key strength of their approach is flexibility. The autoencoder structure can capture a more complex, nonlinear relaionships between different factors. This flexibility is very important in financial market or study since the return of the stock are almost surely not linear, just we are assuming it is for so investigation. Additionally, autoencoder can also handle large and high-dimensional datasets.\
In terms of the drawbacks. As a direct consequence of the increase in model complixity, it makes it difficult to understand what is actually happening behind these models. Although the authors provide variable importance rankings to highlights which factors are more influential, the black-box nature of machine learning especially deep learning can obscure the economic intuition behind specific predictions.And another drawbacks will be its running time due to the complexity.\
In terms of some solutions, I am thinking about using other machine learning methods (yet I don't know what kind of) to study the contribution of certain factors so that it can somehow help us to interpret the autoencoder model, to improve a bit interpretabiltiy. 



\newpage
#Q2
```{r, message = FALSE, warning = FALSE}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(keras)){install.packages("keras")}
if(!require(patchwork)){install.packages("patchwork")}
```

```{r, message = FALSE, warning = FALSE}
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package
load("data_ml.RData")                   # Load the data
data_ml <- data_ml %>% 
    filter(date > "1999-12-31",         # Keep the date with sufficient data points
           date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```

```{r, message = FALSE, warning = FALSE}
stock_ids <- levels(as.factor(data_ml$stock_id)) # A list of all stock_ids
stock_days <- data_ml %>%                        # Compute the number of data points per stock
    group_by(stock_id) %>% summarize(nb = n()) 
stock_ids_short <- stock_ids[which(stock_days$nb == max(stock_days$nb))] # Stocks with full data
returns <- data_ml %>%                           # Compute returns, in matrix format, in 3 steps:
    filter(stock_id %in% stock_ids_short) %>%    # 1. Filtering the data
    dplyr::select(date, stock_id, R1M_Usd) %>%   # 2. Keep returns along with dates & firm names
    spread(key = stock_id, value = R1M_Usd)      # 3. Put in matrix shape 
features <- colnames(data_ml[3:95]) # Keep the feature's column names (hard-coded, beware!)
features_short <- c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", 
                    "Ocf", "Pb", "Vol1Y_Usd")
data_ml <- data_ml %>% 
    group_by(date) %>%                                   # Group by date
    mutate(R1M_Usd_C = R1M_Usd > median(R1M_Usd),        # Create the categorical labels
           R12M_Usd_C = R1M_Usd > median(R12M_Usd)) %>%
    ungroup() %>%
    mutate_if(is.logical, as.factor)
separation_date <- as.Date("2014-01-15")
training_sample <- filter(data_ml, date < separation_date)
testing_sample <- filter(data_ml, date >= separation_date)
```

```{r, message = FALSE, warning = FALSE}
data_short <- data_ml %>%         # Shorter dataset
    filter(stock_id %in% stock_ids_short) %>%
    dplyr::select(c("stock_id", "date",features_short, "R1M_Usd"))
dates <- unique(data_short$date)  # Vector of dates

N <- length(stock_ids_short)      # Dimension for assets
Tt <- length(dates)               # Dimension for dates
K <- length(features_short)       # Dimension for features

factor_data <- data_short %>%  # Factor side date
    dplyr::select(date, stock_id, R1M_Usd) %>%
    spread(key = stock_id, value = R1M_Usd) %>%
    dplyr::select(-date) %>%
    as.matrix()

beta_data <- array(unlist(data_short %>%  # Beta side data: beware the permutation below!
                              dplyr::select(-stock_id, -date, -R1M_Usd)), 
                   dim = c(N, Tt, K))
beta_data <- aperm(beta_data, c(2,1,3))   # Permutation
```

```{r, message = FALSE, warning = FALSE}
main_input <- layer_input(shape = c(N), name = "main_input")  # Main input: returns      
factor_network <- main_input %>%                              # Def of factor side network
    layer_dense(units = 8, activation = "relu", name = "layer_1_r") %>%
    layer_dense(units = 4, activation = "tanh", name = "layer_2_r") 

aux_input <- layer_input(shape = c(N,K), name = "aux_input")  # Aux input: characteristics
beta_network <- aux_input %>%                                 # Def of beta side network
    layer_dense(units = 8, activation = "relu", name = "layer_1_l") %>%
    layer_dense(units = 4, activation = "tanh", name = "layer_2_l") %>%
    layer_permute(dims = c(2,1), name = "layer_3_l")          # Permutation!

main_output <- layer_dot(c(beta_network, factor_network),     # Product of 2 networks
                         axes = 1, name = "main_output") 

model_ae <- keras_model(                                      # AE Model specs
    inputs = c(main_input, aux_input),
    outputs = c(main_output)
)

summary(model_ae)
```
The model uses two input streams, which are processed separately in the merge. The "aux_input" goes through two dense layers. AT the same time, the "main_input" goes through two dense layers to reduce its previous dimensions from 8 to 4. The outputs of the two sub-networks are then combined using a dot product, producing a final output with a total of 6,488 permutations. 
```{r, message = FALSE, warning = FALSE}
model_ae %>% keras::compile(           # Learning parameters
    optimizer = "rmsprop",
    loss = "mean_squared_error"
)

temp = model_ae %>% fit(                      # Learning function
    x = list(main_input = factor_data, aux_input = beta_data),
    y = list(main_output = factor_data),
    epochs = 20,                      # Nb rounds
    batch_size = 49                   # Nb obs. per round
)
plot(temp)
```
Over 20 epochs, the model’s loss steadily decreases from 0.04 to 0.03, indicating effective learning and stable convergence.

\newpage
#Q3
```{r, message = FALSE, warning = FALSE}
model_ua <- keras_model_sequential()
model_ua %>%   
  layer_dense(units = 16, activation = 'sigmoid', input_shape = 1) %>%
  layer_dense(units = 1) # 
model_ua %>% keras::compile(           
  loss = 'mean_squared_error',               
  optimizer = optimizer_rmsprop(),         
  metrics = c('mean_absolute_error')            # Output metric
)
summary(model_ua)                         
```

The  model has a layer of 16 units and an output layer of 1 unit, for a total of 49 trainable parameters. The relatively small number of parameters makes the model lightweight and efficient, while still being able to learn nonlinear mappings from one-dimensional inputs.

```{r, message = FALSE, warning = FALSE}
fit_ua <- model_ua %>% 
  fit(seq(0, 6, by = 0.001) %>% matrix(ncol = 1), 
      sin(seq(0, 6, by = 0.001)) %>% matrix(ncol = 1), 
      epochs = 30, batch_size = 64
  ) 
plot(fit_ua)
```
Over the 30 training epochs, the model’s loss and mean absolute error both drop, indicating that the model is effectively learning the target function. The rapid drop in the early epochs indicates that the chosen architecture and optimizer are well suited for the task, while the continued downward trend indicates a recovery to stability without significant overfits

```{r, message = FALSE, warning = FALSE}
library(patchwork)
model_ua2 <- keras_model_sequential()
model_ua2 %>% 
  layer_dense(units = 128, activation = 'sigmoid', input_shape = 1) %>%
  layer_dense(units = 1) # 
model_ua2 %>% keras::compile(     
  loss = 'mean_squared_error',  
  optimizer = optimizer_rmsprop(),  
  metrics = c('mean_absolute_error')
)
summary(model_ua2)              
```

The sequential model has one layer with 128 units and an output layer with one unit, for a total of 385 trainable parameters. The larger hidden layer significantly improves the network's ability to learn more complex functions compared to the smaller model. As a result, it may produce more accurate estimates, but requires more computing resources.

```{r, message = FALSE, warning = FALSE}
fit_ua2 <- model_ua2 %>% 
  fit(seq(0, 6, by = 0.0002) %>% matrix(ncol = 1),     
      sin(seq(0, 6, by = 0.0002)) %>% matrix(ncol = 1), 
      epochs = 30, batch_size = 64      
  ) 
tibble(x = seq(0, 6, by = 0.001)) %>%
  ggplot() + 
  geom_line(aes(x = x, y = predict(model_ua, x), color = "Small model")) +
  geom_line(aes(x = x, y = predict(model_ua2, x), color = "Large model")) +
  stat_function(fun = sin, aes(color = "sin(x) function")) + 
  scale_color_manual(values = c("#EEAA33", "#3366CC", "#000000")) + theme_bw()
```
The plot compares two neural networks—one with fewer hidden units (“Small model”) and another with more (“Large model”)—against the true sin(x) function. The larger network (blue line) more closely tracks sin(x), illustrating how increasing model capacity improves approximation accuracy.


```{r, message = FALSE, warning = FALSE}
tibble(x = seq(-6, 12, by = 0.001)) %>%
  ggplot() + 
  geom_line(aes(x = x, y = predict(model_ua, x), color = "Small model")) +
  geom_line(aes(x = x, y = predict(model_ua2, x), color = "Large model")) +
  stat_function(fun = sin, aes(color = "sin(x) function")) + 
  scale_color_manual(values = c("#EEAA33", "#3366CC", "#000000")) + theme_bw()
```
Here, the input range extends beyond the original [0, 6], illustrating how each model behaves outside the primary training region. The larger model (blue line) continues to approximate sin(x) reasonably well, while the smaller model (orange line) diverges more significantly. This demonstrates that the larger model’s higher capacity allows it to generalize better across a wider input domain.
